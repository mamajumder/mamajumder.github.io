hh <- 9
mm <- 5
dd <- '2014-10-17'
hh <- '09'
mm <- '05'
tm <- paste(hh,mm, sep=':')
tm
dt <- paste(dd,tm)
dt
dt <- as.Date(paste(dd,tm), format='%Y-%m-%d %H:%M')
dt
dt <- as.Date(paste(dd,tm), format='%Y-%m-%d %H:%M:%S')
dt
dt <- as.Date(paste(dd,tm), format='%Y-%m-%d %H:%M:')
t
dt
dt <- as.Date(paste(dd,tm), format='%Y-%m-%d %H:%M')
dt
dd <- '2014-10-17'
hh <- '09'
mm <- '05'
ss <- '00'
tm <- paste(hh,mm,ss, sep=':')
tm
dt <- as.Date(paste(dd,tm), format='%Y-%m-%d %H:%M:%S')
dt
dt <- as.Date(paste(dd,tm))
dt
dt <- .POSIXct(paste(dd,tm))
dt
ddt <- '1970-01-01 04:32:32'
ddt
as.Date(ddt)
ymd_hms(ddt)
dt <- paste(dd,tm)
ymd_hms(dt)
dd <- '2014-10-17'
hh <- '09'
mm <- '05'
ss <- '00'
tm <- paste(hh,mm,ss, sep=':')
dt <- paste(dd,tm)
ymd_hms(dt)
hh <- '09'
mm <- '05'
ss <- '00'
tm <- paste(hh,mm,ss, sep=':')
dt <- paste(dd,tm)
x <- ymd_hms(dt)
dd <- '2014-10-17'
hh <- c('09','03')
mm <- c('05','01')
ss <- '00'
tm <- paste(hh,mm,ss, sep=':')
dt <- paste(dd,tm)
x <- ymd_hms(dt)
x
difftime(x[1],x[2])
getUser("Rbloggers")$followersCount
rblog <- getUser("Rbloggers")
str(rblog)
me <- getUser("mmajumder")
me
str(me)
snHashtags = str_extract_all(snDf$text, "#\\w+")
library(stringr)
user <- getUser("drsanjaygupta")
sanjay <- userTimeline(user, n=500)
snDf <- twListToDF(sanjay)
head(snDf$text)
snText <- sapply(sanjay, function(x) x$getText())
snHashtags = str_extract_all(snDf$text, "#\\w+")
snHashtags
library(wordcloud)
freq <- table(snHashtags)
snHashtags = str_extract_all(snDf$text, "#\\w+")
freq <- table(snHashtags)
freq <- table(unlist(snHashtags))
wordcloud(names(freq), freq, random.order=FALSE, colors="#1B9E77")
freq
?searchTwitter
? removeWords
?? removeWords
library(RMySQL)
con = dbConnect(MySQL(),user="training", password="training123",
dbname="trainingDB", host="localhost")
con = dbConnect(MySQL(),user="training", password="training123",
dbname="trainingDB", host="localhost")
dbGetQuery(con, "describe  titanic")
library(knitr)
sessionInfo()
library(knitr)
sessionInfo()
update.packages(ask = FALSE, dependencies = c('Suggests'))
library(packdep)
d = map.depends()
centrality(d, order.by = "betweenness")
install.packages("packdep")
library(packdep)
d = map.depends()
centrality(d, order.by = "betweenness")
cc <- centrality(d, order.by = "betweenness")
head(cc)
install_github("talgalili/installr", username="talgalili")
library(installr)
RStudio_CRAN_data_folder <- download_RStudio_CRAN_data(START = '2013-06-10',
END = '2013-06-17')
my_RStudio_CRAN_data <- read_RStudio_CRAN_data(RStudio_CRAN_data_folder)
wm <- pkgDNLs_worldmapcolor(pkg_name="timeROC",
dataset = my_RStudio_CRAN_data)
wm
library(devtools)
install_github("talgalili/installr", username="talgalili")
library(installr)
RStudio_CRAN_data_folder <- download_RStudio_CRAN_data(START = '2013-06-10',
END = '2013-06-17')
my_RStudio_CRAN_data <- read_RStudio_CRAN_data(RStudio_CRAN_data_folder)
wm <- pkgDNLs_worldmapcolor(pkg_name="timeROC",
dataset = my_RStudio_CRAN_data)
wm
wm <- pkgDNLs_worldmapcolor(pkg_name="ggplot2",
dataset = my_RStudio_CRAN_data)
wm
wm <- pkgDNLs_worldmapcolor(pkg_name="dplyr",
dataset = my_RStudio_CRAN_data)
wm
wm <- pkgDNLs_worldmapcolor(pkg_name="plyr",
dataset = my_RStudio_CRAN_data)
wm
RStudio_CRAN_data_folder <- download_RStudio_CRAN_data(START = '2014-06-10',
END = '2014-06-17')
wm <- pkgDNLs_worldmapcolor(pkg_name="dplyr",
dataset = my_RStudio_CRAN_data)
wm <- pkgDNLs_worldmapcolor(pkg_name="plyr",
dataset = my_RStudio_CRAN_data)
wm
wm <- pkgDNLs_worldmapcolor(pkg_name="ggplot2",
dataset = my_RStudio_CRAN_data)
wm
? readShapePoly
?? readShapePoly
library(maptools)
?? readShapePoly
? readShapePoly
myshape <- unzip('http://www.naturalearthdata.com/http//www.naturalearthdata.com/download/10m/cultural/ne_10m_urban_areas.zip')
myshape <- unzip('www.naturalearthdata.com/download/10m/cultural/ne_10m_urban_areas.zip')
? unzip
myshape <- unzip(readLines('www.naturalearthdata.com/download/10m/cultural/ne_10m_urban_areas.zip'))
paths <- "~/data-sc-hw/natural-earth/ne_50m_rivers_lake_centerlines"
dir(paths)
paths <- "~/data-sc-hw/natural-earth"
dir(paths)
ne_lakes <- readOGR(paste(paths,'ne_50m_lakes.shp',sep='/'),
'ne_50m_lakes')
library(rgdal)
paths <- "~/data-sc-hw/natural-earth"
ne_lakes <- readOGR(paste(paths,'ne_50m_lakes.shp',sep='/'),
'ne_50m_lakes')
dir(paths)
dir(paste(paths,'ne_50m_lakes.shp',sep='/'))
ne_lakes <- readOGR(paste(paths,'ne_50m_lakes/ne_50m_lakes.shp',sep='/'),
'ne_50m_lakes')
dir(paste(paths,'ne_50m_lakes/ne_50m_lakes.shp',sep='/'))
lk_path <- paste(paths,'ne_50m_lakes/ne_50m_lakes.shp',sep='/')
lk_path
dir(lk_path)
dir("~/data-sc-hw/natural-earth/ne_50m_lakes")
lk_path <- paste(paths,'ne_50m_lakes/ne_50m_lakes.shp',sep='/')
ne_lakes <- readOGR(lk_path,'ne_50m_lakes')
library(raster)
install.packages("raster", dependencies = FALSE)
library(raster)
library(reshape2)
library(plyr)
ne_lakes <- readOGR(lk_path,'ne_50m_lakes')
rv_path <- paste(paths,'ne_50m_rivers_lake_centerlines/ne_50m_rivers_lake_centerlines.shp',sep='/')
rv_path
ne_rivers <- readOGR(rv_path,'ne_50m_rivers_lake_centerlines')
dir
dir()
dir("trainingDB.sql")
dir(paths)
lk_path
dir("~/data-sc-hw/natural-earth/ne_50m_lakes")
? readOGR
lk_path <- paste(paths,'ne_50m_lakes/ne_50m_lakes.shp',sep='/')
lk_path
ne_lakes <- readOGR("~/data-sc-hw/natural-earth/ne_50m_lakes/ne_50m_lakes.shp",'ne_50m_lakes')
ne_lakes <- readOGR("~/data-sc-hw/natural-earth/ne_50m_lakes/ne_50m_lakes.shp",
'ne_50m_lakes')
library(maptools)
library(ggplot2)
library(maptools)
library(rgeos)
library(plyr)
oldwd <- getwd()
tmp <- tempdir()
tmp
oldwd
setwd(tmp)
url <- "http://www.naturalearthdata.com/http//www.naturalearthdata.com/download/50m/cultural/50m-admin-0-countries.zip"
dest <- paste(tmp,"\\tmp.zip",sep="")
download.file(url,dest)  #File is 1.3Mb
url <- "http//www.naturalearthdata.com/download/50m/cultural/50m-admin-0-countries.zip"
dest <- paste(tmp,"\\tmp.zip",sep="")
download.file(url,dest)  #File is 1.3Mb
url <- "http//www.naturalearthdata.com/download/50m/cultural/50m-admin-0-countries.zip"
dest <- paste(tmp,"/tmp.zip",sep="")
download.file(url,dest)  #File is 1.3Mb
dest
url
download.file(url,dest)  #File is 1.3Mb
url <- "http://www.naturalearthdata.com/download/50m/cultural/50m-admin-0-countries.zip"
dest <- paste(tmp,"/tmp.zip",sep="")
download.file(url,dest)  #File is 1.3Mb
url <- "https://www.naturalearthdata.com/download/50m/cultural/50m-admin-0-countries.zip"
dest <- paste(tmp,"/tmp.zip",sep="")
download.file(url,dest)  #File is 1.3Mb
rv_path
url <- "http://www.naturalearthdata.com/download/50m/cultural/ne_50m-admin-0-countries.zip"
dest <- paste(tmp,"/tmp.zip",sep="")
download.file(url,dest)  #File is 1.3Mb
url <- "http://www.naturalearthdata.com/download/50m/cultural/ne_50m_admin_0_countries.zip"
dest <- paste(tmp,"/tmp.zip",sep="")
download.file(url,dest)  #File is 1.3Mb
dd <- download.file(url)  #File is 1.3Mb
download.file(url,dest)  #File is 1.3Mb
url <- "http://www.naturalearthdata.com/http//www.naturalearthdata.com/download/50m/cultural/ne_50m_admin_0_countries.zip"
dest <- paste(tmp,"/tmp.zip",sep="")
download.file(url,dest)  #File is 1.3Mb
unzip(dest)
wld <- readShapePoly("ne_50m_admin_0_countries")
wa <- wld[c(21,22,43,44,81,82,84,144,151,158,160,190,195,213),]
wa@data$id <- rownames(wa@data) # Explicitly identifies attribute rows by the .dbf offset
wa.df <- as.data.frame(wa)
gpclibPermit() # required for fortify method if rgeos is unavailable
wa.fort <- fortify(wa, region="id")
wa.gg <- join(wa.fort, wa.df,by="id")
p <- ggplot(wa.gg)+
geom_polygon(aes(long,lat,group=group))+
geom_path(colour="white",aes(long,lat,group=group),size=1.2)+
coord_equal()+
scale_x_continuous(name=expression(paste("Longitude (",degree,")")),limits=c(-18,4),expand=c(0,0))+
scale_y_continuous(name=expression(paste("Latitude (",degree,")")),limits=c(4,17),expand=c(0,0))
print(p)
unlink(tmp,recursive=T)
setwd(oldwd)
str(wld)
class(wld)
head(wld)
wld[1]
wa <- wld[c(21,22,43,44,81,82,84,144,151,158,160,190),]
wa@data$id <- rownames(wa@data) # Explicitly identifies attribute rows by the .dbf offset
wa.df <- as.data.frame(wa)
gpclibPermit() # required for fortify method if rgeos is unavailable
wa.fort <- fortify(wa, region="id")
wa.gg <- join(wa.fort, wa.df,by="id")
#	Create the plot
p <- ggplot(wa.gg)+
geom_polygon(aes(long,lat,group=group))+
geom_path(colour="white",aes(long,lat,group=group),size=1.2)+
coord_equal()+
scale_x_continuous(name=expression(paste("Longitude (",degree,")")),limits=c(-18,4),expand=c(0,0))+
scale_y_continuous(name=expression(paste("Latitude (",degree,")")),limits=c(4,17),expand=c(0,0))
print(p)
wa <- wld[c(21,22,43,44,81,82,84,144,151,158,160,190,195,213),]
wa@data$id <- rownames(wa@data) # Explicitly identifies attribute rows by the .dbf offset
wa.df <- as.data.frame(wa)
gpclibPermit() # required for fortify method if rgeos is unavailable
wa.fort <- fortify(wa, region="id")
wa.gg <- join(wa.fort, wa.df,by="id")
#	Create the plot
p <- ggplot(wa.gg)+
geom_polygon(aes(long,lat,group=group))+
geom_path(colour="white",aes(long,lat,group=group),size=1.2)+
coord_equal()+
scale_x_continuous(name=expression(paste("Longitude (",degree,")")),limits=c(-18,4),expand=c(0,0))+
scale_y_continuous(name=expression(paste("Latitude (",degree,")")),limits=c(4,17),expand=c(0,0))
print(p)
wa <- wld[c(21,22,43,44,81,82,84,144,151,158,160,190,195,213),]
wld <- readShapePoly("ne_50m_admin_0_countries")
oldwd <- getwd()
tmp <- tempdir()
setwd(tmp)
tmp <- tempdir()
tmp
http://www.naturalearthdata.com/download/50m/cultural/ne_50m_admin_0_countries.zip
oldwd <- getwd()
oldwd
tmp <- tempdir()
setwd(tmp)
tmp
setwd(tmp)
oldwd <- getwd()
tmp <- tempdir()
setwd(tmp)
setwd(tmp)
tmp
stateDf <- read.csv("~/data/state_names.csv")
popDat <- readRDS("~/data/usPop1969_2012.rds")
head(stateDf)
head(popDat)
popDat <- merge(popDat, stateDf, by="State", all.x=TRUE)
head(popDat)
popDat <- popDat %>%
group_by(Year, State_name) %>%
summarize(totPop=sum()) %>%
mutate(State = tolower(State_name))
library(dplyr)
popDat <- popDat %>%
group_by(Year, State_name) %>%
summarize(totPop=sum()) %>%
mutate(State = tolower(State_name))
head(popDat)
popDat <- readRDS("~/data/usPop1969_2012.rds")
head(popDat)
popDat <- popDat %>%
group_by(Year, State) %>%
summarize(totPop=sum(Population))
myPopDat <- merge(popDat, stateDf, by="State", all.x=TRUE)
myPopDat <- myPopDat %>%
select(Year,totPop,State_name) %>%
mutate(state = tolower(State_name))
View(myPopDat)
myPopDat <- myPopDat %>%
select(Year,totPop,State_name) %>%
mutate(State = tolower(State_name))
head(dat)
dat <- read.csv("~/Box Sync/help/data-sets/CrimeStatebyState.csv")
head(dat)
myPopDat <- merge(popDat, stateDf, by="State", all.x=TRUE)
myPopDat <- myPopDat %>%
select(Year<2006,totPop,State_name) %>%
mutate(State = tolower(State_name))
head(myPopDat)
myPopDat <- myPopDat %>%
select(Year,Year<2006,totPop,State_name) %>%
mutate(State = tolower(State_name))
myPopDat <- merge(popDat, stateDf, by="State", all.x=TRUE)
myPopDat <- myPopDat %>%
select(Year,Year<2006,totPop,State_name) %>%
mutate(State = tolower(State_name))
head(myPopDat)
table(myPopDat$Year)
head(dat)
crimeDat <- merge(subset(dat,Year>1968), myPopDat, by=State)
crimeDat <- merge(subset(dat,Year>1968), myPopDat, by="State")
myPopDat <- merge(popDat, stateDf, by="State", all.x=TRUE)
myPopDat <- myPopDat %>%
select(Year,Year<2006,totPop,State_name) %>%
mutate(state = tolower(State_name))
dat <- read.csv("~/Box Sync/help/data-sets/CrimeStatebyState.csv")
dat$state <- tolower(dat$State)
crimeDat <- merge(subset(dat,Year>1968), myPopDat, by="state")
head(crimeDat)
crimeDat <- merge(subset(dat,Year>1968), myPopDat, by=c("state","Year"))
head(crimeDat)
crimeDat <- merge(subset(dat,Year>1968), myPopDat, by=c("state","Year")) %>%
select(state,Year, Type.of.Crime,Crime, Count, totPop) %>%
mutate(rate = Count/totPop)
head(crimeDat)
crimeDat <- merge(subset(dat,Year>1968), myPopDat, by=c("state","Year")) %>%
select(state,Year, Type.of.Crime,Crime, Count, totPop) %>%
mutate(rate = Count*1000/totPop)
head(crimeDat)
hist(cimeDat$rate)
hist(crimeDat$rate)
crimeDat <- merge(subset(dat,Year>1968), myPopDat, by=c("state","Year")) %>%
select(state,Year, Type.of.Crime,Crime, Count, totPop) %>%
mutate(rate = Count*10000/totPop)
head(crimeDat)
mdat <- map_data("state")
library(ggmap)
mdat <- map_data("state")
shiny::runApp('Box Sync/help/Shiny/crime-usa')
shiny::runApp('Box Sync/help/Shiny/crime-usa')
subDat <- subset(crimeDat,state=='florida')
ggplot(subDat, aes(x=Year, y=rate,color=Crime)) +
theme_bw()
ggplot(subDat, aes(x=Year, y=rate,color=Crime)) + geom_line()+
theme_bw()
shiny::runApp('Box Sync/help/Shiny/crime-usa')
shiny::runApp('Box Sync/help/Shiny/crime-usa')
shiny::runApp('Box Sync/help/Shiny/crime-usa')
shiny::runApp('Box Sync/help/Shiny/crime-usa')
shiny::runApp('Box Sync/help/Shiny/crime-usa')
shiny::runApp('Box Sync/help/Shiny/crime-usa')
shiny::runApp('Box Sync/help/Shiny/crime-usa')
shiny::runApp('Box Sync/help/Shiny/crime-usa')
shiny::runApp('Box Sync/help/Shiny/crime-usa')
shiny::runApp('Box Sync/help/Shiny/crime-usa')
shiny::runApp('Box Sync/help/Shiny/crime-usa')
shiny::runApp('Box Sync/help/Shiny/crime-usa')
shiny::runApp('Box Sync/help/Shiny/crime-usa')
shiny::runApp('Box Sync/help/Shiny/crime-usa')
shiny::runApp('Box Sync/help/Shiny/crime-usa')
shiny::runApp('Box Sync/help/Shiny/crime-usa')
txts <- '<div class="entry-content">
:before
<div style="text-align:left;">...</div>
<div style="text-align:left;">...</div>
<div style="text-align:left;"></div>
<div style="text-align:left;">...</div>
<div style="text-align:left;">...</div>
<div style="text-align:left;">...</div>
<ul>...</ul>
<p>...</ul>
</div>'
</div>')
shiny::runApp('Box Sync/help/Shiny/crime-usa')
shiny::runApp('Box Sync/help/Shiny/crime-usa')
shiny::runApp('Box Sync/help/Shiny/crime-usa')
shiny::runApp('Box Sync/help/Shiny/crime-usa')
shiny::runApp('Box Sync/help/Shiny/crime-usa')
shiny::runApp('Box Sync/help/Shiny/crime-usa')
shiny::runApp('Box Sync/help/Shiny/crime-usa')
shiny::runApp('Box Sync/help/Shiny/crime-usa')
shiny::runApp('Box Sync/help/Shiny/crime-usa')
shiny::runApp('Box Sync/help/Shiny/crime-usa')
shiny::runApp('Box Sync/help/Shiny/crime-usa')
shiny::runApp('Box Sync/help/Shiny/crime-usa')
shiny::runApp('Box Sync/help/Shiny/crime-usa')
shiny::runApp('Box Sync/help/Shiny/crime-usa')
shiny::runApp('Box Sync/help/Shiny/crime-usa')
shiny::runApp('Box Sync/help/Shiny/crime-usa')
subDat <- subset(crimeDat,state=='florida')
ggplot(subDat, aes(x=Year, y=rate,color=Crime)) + geom_line()+
theme_bw() +
scale_fill_continuous(low="blue", high="pink") +
title(titleText)
titleText=1
ggplot(subDat, aes(x=Year, y=rate,color=Crime)) + geom_line()+
theme_bw() +
scale_fill_continuous(low="blue", high="pink") +
title(titleText)
head(subDat)
ggplot(subDat, aes(x=Year, y=rate,color=Crime)) + geom_line()+
theme_bw() + title(titleText)
ggplot(subDat, aes(x=Year, y=rate,color=Crime)) + geom_line()+
theme_bw() + ggtitle(titleText)
shiny::runApp('Box Sync/help/Shiny/crime-usa')
shiny::runApp('Box Sync/help/Shiny/crime-usa')
rm()
sPath <- "~/data/us.1969_2012.19ages.adjusted.txt"
widths <- c(4,2,2,3,2,1,1,1,2,8)
popDat <- read.fwf(sPath,widths)
colnames(popDat) <- c("Year", "State", "State_c", "County","Registry",
"race","Origin","Sex","Age","Population")
library(XML)
statePath <- "http://www.mcc.co.mercer.pa.us/dps/state_fips_code_listing.htm"
tables <- readHTMLTable(statePath)[1]
tbl1 <- tables[[1]][2:28,1:3]
colnames(tbl1) <- c("State", "State_c", "State_name")
tbl2 <- tables[[1]][2:29,4:6]
colnames(tbl2) <- c("State", "State_c", "State_name")
stateDf <- rbind(tbl1, tbl2)
sPopDat <- popDat %>%
group_by(Year, State) %>%
summarize(totPop=sum(Population))
myPopDat <- merge(sPopDat, stateDf, by="State", all.x=TRUE)
myPopDat <- myPopDat %>%
select(Year,Year<2006,totPop,State_name) %>%
mutate(state = tolower(State_name))
dat <- read.csv("~/Box Sync/help/data-sets/CrimeStatebyState.csv")
dat$state <- tolower(dat$State)
crimeDat <- merge(subset(dat,Year>1968), myPopDat, by=c("state","Year")) %>%
select(state,Year, Type.of.Crime,Crime, Count, totPop) %>%
mutate(rate = Count*10000/totPop)
write.csv(crimeDat,"~/data/usaCrimeDat.csv")
write.csv(crimeDat,"~/data/usaCrimeDat.csv", row.names=FALSE)
write.rds(crimeDat,"~/data/usaCrimeDat.rds")
saveRDS(crimeDat,"~/data/usaCrimeDat.rds")
crimeDat <- readRDS("~/data/usaCrimeDat.rds")
head(crimeDat)
shiny::runApp('Box Sync/help/Shiny/crime-usa')
shiny::runApp('Box Sync/help/Shiny/crime-usa')
shiny::runApp('Box Sync/help/Shiny/crime-usa')
shiny::runApp('Box Sync/help/Shiny/crime-usa')
shiny::runApp('Box Sync/help/Shiny/crime-usa')
shiny::runApp('Box Sync/help/Shiny/crime-usa')
shiny::runApp('Box Sync/help/Shiny/crime-usa')
shiny::runApp('Box Sync/help/Shiny/crime-usa')
shiny::runApp('Box Sync/help/Shiny/crime-usa')
shiny::runApp('Box Sync/help/Shiny/crime-usa')
shiny::runApp('Box Sync/help/Shiny/crime-usa')
shiny::runApp('Box Sync/help/Shiny/crime-usa')
shiny::runApp('Box Sync/help/Shiny/crime-usa')
library(rmarkdown)
setwd("~/Box Sync/Teaching/stat4410-8416-Data-Science/lectures/19-interactive-shiny-ggvis")
dir()
library(ggvis)
mtcars %>%
ggvis(~wt, ~mpg, size := input_slider(10, 1000)) %>%
layer_points(fill := "red") %>%
layer_points(stroke := "black", fill := NA)
mtcars %>%
ggvis(~wt, ~mpg) %>%
layer_smooths(span = input_slider(0.5, 1, value = 1)) %>%
layer_points(size := input_slider(100, 1000, value = 100))
mtcars %>% ggvis(x = ~wt) %>%
layer_densities(
adjust = input_slider(.1, 2, value = 1, step = .1, label = "Bandwidth adjustment"),
kernel = input_select(
c("Gaussian" = "gaussian",
"Epanechnikov" = "epanechnikov",
"Rectangular" = "rectangular",
"Triangular" = "triangular",
"Biweight" = "biweight",
"Cosine" = "cosine",
"Optcosine" = "optcosine"),
label = "Kernel")
)
